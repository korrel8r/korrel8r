=  Korrel8r as an AI tool
:link-openapi-mcp: shttps://github.com/jedisct1/openapi-mcp
:link-mcp: https://modelcontextprotocol.io/
:link-korrel8r: https://korrel8r.github.io/korrel8r/
:toc: left

link:{link-mcp}[Model Context Protocol] (MCP) allows LLMs to use external tools.
This is an experiment in using link:{link-korrel8r}[Korrel8r] as an MCP tool.

CAUTION: This is an experiment in progress. Everything is subject to change without notice.

link:{link-openapi-mcp}[openapi-mcp] creates an MCP server from an OpenAPI spec.
The MCP server exposes the API description so that models can learn how to use the tool.
Korrel8r is a REST service so we can use `openapi-mcp` to create an MCP "wrapper".

IMPORTANT: You must `oc login` and be logged in to an Openshift cluster.


== Run Korrel8r

The Cluster Observability Operator (COO) deploys korrel8r for you in the cluster.
To use the COO korrel8r instance, set its URL in your environment:

  export KORREL8R_URL=$(oc get routes/korrel8r -n openshift-cluster-observability-operator -o template='https://{{.spec.host}}')

You can also run korrel8r locally, using routes to connect to stores in the cluster.

  korrel8r web --http :8080 --config ../etc/korrel8r/openshift-route.yaml
  export KORREL8R_URL=http://localhost:8080

== Generating an MCP server from the REST API

=== Install openapi-mcp

[,terminal]
----
go install github.com/jedisct1/openapi-mcp/cmd/openapi-mcp@latest
----

The MCP server can run in two modes:
- as a "stdio" server, intended to be run by a local AI proxy.
- as a stand-alone HTTP proxy, receiving MCP requests and forwarding REST calls to korrel8r.

This shell script runs the MCP server:

.korrel8r-mcp.sh
[,bash]
----
include::korrel8r-mcp.sh[]
----

NOTE: This script runs openapi-mcp as a `stdio` server for experimentation purposes.
A more realistic deployment would run a HTTP MCP server in the cluster beside korrel8r.

=== MCP configuration

The following configuration tells MCP clients (AI model proxies) how to run `korrel8r-mcp.sh`
as an MCP server in `stdio` mode.

.mcpconfig.json
[,json]
----
{
  "mcpServers": {
    "korrel8r": {
      "command": "</abs/path/to/>korrel8r-mcp.sh"
    }
  }
}
----

== Claude Desktop

Claude desktop is a local proxy for the claude.ai model.
It was the first experiment because it's widely advertised to work with local MCP servers.

=== Installing on Fedora 42

Claude-desktop is not officially packaged for Linux, but there is an unofficial fedora package.
Clone the repository and follow README to build & install.

[,terminal]
----
git clone https://github.com/sneered/claude-desktop-fedora.git
----

This didn't immediately work, there were errors about GT 3/4 version clashes.
Installing an *older* version of electron (the portable runtime that Claude uses) fixed the problem.

[,terminal]
----
sudo npm remove -g electron
sudo npm install -g electron@v35.5.1
----

=== Configure

Copy the configuration file (or edit the Claude configuration file) as follows:

[,terminal]
----
co mcpconfig.json ~/.config/Claude/claude_desktop_config.json
----

IMPORTANT: If you modify the configuration or the `korrel8r-mcp.sh` script,
exit Claude desktop with *File > Exit*  (not *File > Close*) and re-start.

=== Results

- Finds the korrel8r tool and is able to read and report the available endpoints.
- Successfully calls `listDomains` (no parameters) and describes the available domains.
- Tries very hard to make queries
  - tries each endpoint available, many formats.
  - cannot format korrel8r queries correctly
- Halucinates a graph out of nothing

.Results
- Claude does query the MCP info and try to use each of the API endpoints.
- Successfully lists domains, and describes them sensibly.
- Did not make any successful queries. The korrel8r query syntax is not widely discussed on the internet.
- Hit usage restrictions of free Claude very quickly, it is not a practical tool for further experiments.

== Openshift Lightspeed

Openshift Lightspeed integrates a chatbot into the Openshift console. Install as follows:

. From Operatorhub, install the 'OpenShift Lightspeed Operator'
. Configure lightspeed and a secret with your OpenAI API token.
+
----
SECRET=<your-token-string> envsubst < credentials.yaml | oc apply -f -  -f olsconfig.yaml
----
. Scale down the operator so you can manally edit the configmap. +
  `kubectl scale --replicas=0 -n openshift-lightspeed deployment/lightspeed-operator-controller-manager`
. Edit the config file and add this section under `ols_config`
+
----
include::olsconfig-cm-patch.yaml[]
----

[WARNING]
====
Incomplete, had problems with access quotas to OpenAI. To be continued.

F  oc logs -n openshift-lightspeed deployment/lightspeed-app-server

  2025-07-22 18:19:03,002 [ols.app.endpoints.health:health.py:59] ERROR: LLM connection check failed with - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
====

== Next steps

- Improve documentation and examples of the query format available via MCP `info` - i.e. the REST API `description` and `example` elements
- Add more HTTP links to the korrel8r website in the API doc, so models can follow those links and use the doc there.
- Use Red Hat light-speed to drive the experiment instead of public models.
- Write a dedicated MCP server rather than generating from REST. Reasons:
- Enable interaction between console, korrel8r & AI.
  - Let AI change the console page (Troubleshooting-Panel? Korrel8r? Both?)
  - Use Korrel8r as an entry point for AI to query all back-end stores.
  - Provide a way for LLMs to refer to resources/signals/console pages - korrel8r query format?
  - Inject console context into chat context.

== Debugging tricks

The following command install and runs an interactive browser-based tool that can connect to any MCP server.
It will show metadata, let you call tools interactively and examine the responses.

A browser-based tool that can connect to any MCP server.
It will show metadata and allow you to experiment with calling tools.
[,terminal]
----
npx @modelcontextprotocol/inspector
----

IMPORTANT: Open the URL marked "Open inspector with token pre-filled:" in your browser.
